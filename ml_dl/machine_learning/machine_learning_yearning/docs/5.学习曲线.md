[目录]
---

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [诊断偏差与方差：学习曲线](#诊断偏差与方差学习曲线)
- [绘制训练误差曲线](#绘制训练误差曲线)
- [解读学习曲线](#解读学习曲线)
  - [高可避免偏差](#高可避免偏差)
  - [高方差](#高方差)
  - [高方差 & 高偏差](#高方差-高偏差)
- [绘制学习曲线](#绘制学习曲线)

<!-- /code_chunk_output -->

# 诊断偏差与方差：学习曲线

我们现在已经知道了一些方法，来估计可避免多少由方差和偏差导致的误差——通常会估计最优错误率，以及算法在训练集和测试集上的误差。下面讨论一种更有帮助的方式——绘制学习曲线

学习曲线将训练集样本数量和开发集的误差关联起来。

**开发集的误差随着训练集大小变化的曲线就叫做学习曲线。**

![](/images/Untitled.png)

随着训练集的增加，开发集的误差应该降低。

通常，我们会设置一些“期望误差率”，并希望算法最终能达到该值。例如：

- 如果希望算法能达到人类水平的表现，那么人类错误率可能就是“期望错误率”。
- 如果学习算法为某些产品提供服务（如提供猫咪图片），我们或许将主观感受到需什么样的水平才能给用户提供出色的体验。
- 如果你已经从事一项应用很长时间，那么你可能会有一种直觉，预判在下一个季度里你会有多大的进步。

将期望的水平添加到学习曲线中：

![](/images/Untitled1.png)

显而易见，你可以根据红色的“开发误差”曲线的走势来推测，在添加一定量的数据后，曲线距离期望的性能接近了多少。在上面的例子中，将训练集的大小增加一倍可能会让你达到期望的性能，这看起来是合理的。

观察学习曲线或许能帮到你，避免花费几个月的时间去收集两倍的训练数据，到头来却发现这并不管用的情况。

该过程的一个缺点是，如果你只关注了开发错误曲线，当数据量变得越来越多时，将很难预测后续红色曲线的走向。因此我们会选择另外一条曲线来协助评估添加数据所带来的影响:  即训练误差曲线。

# 绘制训练误差曲线

随着训练集大小的增加，开发集（和测试集）误差应该会降低，但你的训练集误差往往会随之增加。

让我们来举例说明一下。假设你的训练集只有两个样本：一张猫图和一张非猫图。学习算法很轻易地就可以“记住”训练集中这两个样本，并且训练集错误率为 0%. 即使有一张或两张的样本图片被误标注了，算法也能够轻松地记住它们。

现在假设你的训练集包含 100 个样本，其中有一些样本可能被误标记，或者是模棱两可的（图像非常模糊），所以即使是人类也无法分辨图中是否有一只猫。此时，或许学习算法仍然可以“记住”大部分或全部的训练集，但很难获得 100% 的准确率。通过将训练集样本数量从 2个增加到 100 个，你会发现训练集的准确率会略有下降。

下面我们将训练误差曲线添加到原有的学习曲线中：

![](/images/Untitled2.png)

可以看到，蓝色的“训练误差”曲线随着训练集大小的增加而上升，而且算法在训练集上通常比在开发集上表现得更好；因此，**红色的开发误差曲线通常严格位于蓝色训练错误曲线之上**。

# 解读学习曲线

## 高可避免偏差

假设你的开发误差曲线如下所示：

![](/images/Untitled3.png)

我们之前提到，如果开发误差曲线趋于平稳，则不太可能通过添加数据来达到预期的性能，但也很难确切地知道红色的开发错误曲线将趋于何值。如果开发集很小，或许会更加不确定，因为曲线中可能含有一些噪音干扰。

假设我们把训练误差曲线加到上图中，可以得到如下结果：

![](/images/Untitled4.png)

现在可以绝对肯定地说，添加更多的数据并不奏效。为什么呢？记住我们的两个观察结果：

- 训练数据越大，训练误差也越大。因此蓝色训练误差曲线只会不动或上升
- 红色的开发误差曲线通常高于蓝色训练误差曲线。因此只要训练误差高于期望性能

，通过添加更多数据来让红色开发误差曲线下降到期望性能水平之下也基本没有可能。在同一张图中检查开发误差曲线和训练误差曲线可以让我们对推测开发误差曲线的走势更有信心。

为了便于讨论，假设期望性能是我们对最优错误率的估计。那么上面的图片就是一个标准的“教科书”式的例子（具有高可避免偏差的学习曲线是什么样的）：在训练集大小的最大处（大致对应使用我们所有的训练数据），训练误差和期望性能之间有较大的间隙，这代表可避免偏差较大。此外，如果训练曲线和开发曲线之间的间隙小，则表明方差小。

之前，我们只在曲线最右端的点去衡量训练集误差和开发集误差，这与使用所有的可训练数据训练算法相对应。绘制完整的学习曲线将为我们呈现更全面的结果图片，显示算法在不同训练集大小上的表现。

## 高方差

考虑下面的学习曲线：

![](/images/Untitled5.png)

蓝色训练误差曲线相对较低，红色的开发误差曲线比蓝色训练误差高得多。因此，偏差很小，但方差很大。添加更多的训练数据可能有助于缩小开发误差和训练误差之间的差距。

## 高方差 & 高偏差

考虑下面的情况：

![](/images/Untitled6.png)

这种情况下，训练误差很大，它比期望的性能水平要高得多，开发误差也比训练误差大得多。因此，学习算法有着明显的偏差和方差。此时你必须找到一种方法来减少算法中的偏差和方差。

# 绘制学习曲线

假设你的训练集非常小，仅有100个样本，那么绘制学习曲线时，你可以每次随机增加10个样本来训练你的算法，但这样做会有风险：

当你只使用10个随机选择的样本进行训练时，你可能会不幸的碰到特别糟糕的训练集，比如含有很模糊的或者误标记的样本。你当然也有可能会幸运地碰到特别“棒”的训练集。训练集的规 模较小意味着开发和训练误差将随机波动。

如果你的机器学习应用程序很倾向于某一个类（如猫分类任务的负面样本比例远远大于正面样本），或者说有大量的类（如识别 100 种不同的动物物种），那么选择一个“非代表性”或糟糕的特殊训练集的几率也将更大 。例如，假设你的整个样本中有 80% 是负样本（y=0），只有20% 是正样本（y=1），那么一个含有 10 个样本的训练集就有可能只包含负样本，因而算法很难从中学到有意义的东西。

存在训练集噪声致使难以正确理解曲线的变化时，有两种解决方案：

- 与其只使用 10 个样本训练单个模型，不如从你原来的 100 个样本中进行随机有放回抽样10，选择几批（比如 3-10 ）不同的 10 个样本进行组合。在这些数据上训练不同的模型，并计算每个模型的训练和开发错误。最终，计算和绘制平均训练集误差和平均发集误差。
- 如果你的训练集偏向于一个类，或者它有许多类，那么选择一个“平衡”子集，而不是从100 个样本中随机抽取 10 个训练样本。例如，你可以确保这些样本中的 2/10是正样本，8/10 是负样本。更常见的做法是，确保每个类的样本比例尽可能地接近原始训练集的总体比例。

除非你已经尝试过绘制学习曲线，并得出了曲线太过嘈杂且无法看到潜在趋势的结论，否则我
将不会考虑使用这两种技术。因为当你的训练集规模很大——比如超过 10000 个样本——而 且类分布不是很倾斜时，你可能就不需要这些技巧了

最后提一点，绘制一个学习曲线的成本可能非常高：例如，你可能需要训练 10 个模型，其中样本规模可以是 1000 个，然后是 2000 个，一直到 10000 个。使用小数据集训练模型比使用大型数据集要快得多。因此，你可以用 1000、2000、4000、6000 和 10000 个样本来训练模型，而不是像上面那样将训练集的大小均匀地间隔在一个线性的范围内。这仍然可以让你对学 习曲线的变化趋势有一个清晰的认识。当然，这种技术只有在训练所有额外模型所需的计算成本很重要时才有意义。