[目录]
---

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [开发集与测试集的定义](#开发集与测试集的定义)
- [开发集和测试集应该服从同一分布](#开发集和测试集应该服从同一分布)
- [开发集和测试集的大小](#开发集和测试集的大小)
- [使用单值评估指标进行优化](#使用单值评估指标进行优化)
- [优化指标和满意度指标](#优化指标和满意度指标)
- [通过开发集和度量指标加速迭代](#通过开发集和度量指标加速迭代)
- [何时修改开发集、测试集和指标](#何时修改开发集-测试集和指标)
- [小结](#小结)

<!-- /code_chunk_output -->

# 开发集与测试集的定义

继续分析之前提到的猫咪图片的案例：现在你负责运营一个移动端app，用户会向这个app上传许多不同内容的图片。而你希望这个app能够从图片中自动找到含有猫的图片。

你的团队已经在不同的网站下载了含有猫的图片（正样本/正例），以及不含猫的图片（负样本/反例），从而得到了一个巨大的数据集，他们讲数据集按照70% / 30%的比例划分为训练集和测试集，并且使用这些数据构建出了一个在训练集和测试集上均表现良好的猫咪检测器。

可当你将这个分类器部署到移动应用中时，却发现它的性能相当之差！

这究竟是什么原因导致的呢？

你会发现，从网站上下载下来作为训练集的图片和用户上传的图片有较大的区别——用户上传的图片大部分是手机拍摄，此类图片往往分辨率低，模糊不清，采光也不够理想。但由于用来训练和测试的数据集图片均取自网站，这就导致了算法那不能够很好的泛化到我们所关心的手机图片的实际分布情况上。

在大数据时代来临前，机器学习中的普遍做法是使用70% / 30% 的比例来随机划分训练集和测试集。这种做法的确可行，但在越来越多的实际应用中，训练数据集的分布与人们最终所关心的分布情况往往不同，此时执意采取这样的划分其实是一个坏主意。

通常我们认为：

- **训练集**：用于运行你的学习算法
- **开发集**：用于调整参数，选择特征，以及对学习算法做出其他决定。也称为留出交叉验证集
- 测试集：用于评估算法的性能，但不会据此改变学习算法或参数

在定义了开发集合测试集后，就可以调整学习算法的参数来探索哪些参数的使用效果最好。

所以你应该这样处理：
- 合理的选择开发集和测试集，使之能够代表将来实际数据的情况


# 开发集和测试集应该服从同一分布

根据公司的核心市场分布情况，你将猫咪app 的图像数据划分为美国、中国、印度和其他地区四个区域。在设立开发集和测试集时，你尝试将美国和印度的数据归于开发集，而中国和其他地区的数据归于测试集，这样做对吗？

当然不对！

一旦定义好了，你的团队将专注于提升开发集的性能表现，这就要求开发集能够体现核心任务：使算法在四个地区都表现优异，而不仅仅是其中的两个。

开发集和测试集的分布不同还将导致第二个问题：你的团队所开发的系统可能在开发集上表现良好，却在测试集上表现不佳。我曾目睹过这样的事件，这令人十分沮丧并且还会浪费大量的时间，因此希望你不要重蹈他们的覆辙。

举个例子，假设你的团队开发了一套能在开发集上运行性能良好，却在测试集上效果不佳的系统。如果此时开发集和测试集的分布相同，那么你就能清楚地明白问题所在：算法在开发集上过拟合了（overfit）。解决方案显然就是去获取更多的开发集数据。

但是如果开发集和测试集服从不同的分布，解决方案就不那么明确了。此时可能存在以下一种
或者多种情况：
- 算法在开发集上过拟合了
- 测试集比开发集更难进行预测，尽管算法做得足够好了，却很难有进一步的提升空间
- 测试集不一定更难预测，但它与开发集性质并不相同（分布不同）。因此在开发集上表现良好的算法不一定在测试集上也能够取得出色表现。如果是这种情况，大量针对开发集性能的改进工作将会是徒劳的

如果你想要在特定的机器学习应用上取得进展，而不是搞研究，我建议你尽可能地选择服从相同分布的开发集和测试集数据，这会让你的团队更有效率

# 开发集和测试集的大小

**开发集的规模应该尽可能的大**，至少要能够区分出你所尝试的不同算法之间的性能差异。例如，如果分类器 A 的准确率为 90.0% ，而分类器 B 的准确率为 90.1% ，那么使用仅含有 100个样本的开发集将无法检测出这 0.1% 的差异。与我所遇到的机器学习问题相比，一个样本容量仅为 100 的开发集，规模太小了。通常来说，开发集的规模应该在 1,000 到 10,000 个样本数据之间，而当开发集样本容量为 10,000 时，你将很有可能检测到这 0.1% 的性能提升


在类似广告服务、网络搜索和产品推荐等较为成熟且关键的应用领域，我曾见过一些团队非常积极地去改进算法性能，哪怕只有 0.01% 的提升，因为这将直接影响到公司的利润。在这种情况下，开发集规模可能远超过 10,000 个样本，从而有利于检测到那些不易察觉的效果提升

那么测试集的大小又该如何确定呢？它的规模应该大到使你能够对整体系统的性能进行一个高度可信的评估。一种常见的启发式策略是将整体 30% 的数据用作测试集，这适用于总体数据量规模一般的情况（比如 100 至 10,000 个样本）。但是在大数据时代，有时我们所面临的机器学习问题的样本数量将超过 10 个亿，即使开发集和测试集中样本的绝对数量一直在增长，可总体上分配给开发集和测试集的数据比例正在不断降低。可以看出，我们并不需要将开发集和测试集的规模提升到远远超过评估算法性能所需的程度，也就是说，开发集和测试集的规模并不是越大越好。

# 使用单值评估指标进行优化
所谓的**​单值评估指标**（single-number evaluation metric）​有很多，分类准确率就是其中的一种：待你在开发集（或测试集）上运行分类器之后，它将返回单个数值，代表着样本被正确分类的比例。根据这个指标，如果分类器 A 的准确率为 97％，而分类器 B 的准确率为 90%，那么我们可以认为分类器 A 更优秀。

查准率：已经被预测为猫的图片中，实际类别是猫的样本比例
查全率：实际类别是猫的图片中，被正确预测为猫的样本比例

相比之下，​查准率（Precision，又译作精度）​和​查全率（Recall，又译作召回率）​的组合并不能作为单值评估指标，因为它给出了两个值来对你的分类器进行评估。多值评估指标提高了在算法之间进行优劣比较的难度，假设你的算法表现如下：

classifier|percision|recall
:----:|:----:|:----:
A|95%|90%
B|98%|85%

若根据上方表格中的数值对两个分类器进行比较，显然二者都没有较为明显的优势，因此也无法指导你立即做出选择。

当你的团队在进行开发时，往往会尝试多种多样的算法架构、模型参数、特征选择，或是一些其它的想法。你可以通过使用单值评估指标（如准确率），根据所有的模型在此指标上的表现，进行排序，从而能够快速确定哪一个模型的性能表现最好

如果你认为查准率和查全率指标很关键，可以参照其他人的做法，将这两个值合并为一个值来表示。例如取二者的平均值，或者你可以计算 “F1分数（F1 score）” ，这是一种经过修正的平均值计算方法，比起直接取平均值的效果会好一些。

F1分数：$\frac {2} {\frac {1}{precision} + \frac {1}{recall}}$


classifier|percision|recall|f1score
:----:|:----:|:----:|:----:
A|95%|90%|92.4%
B|98%|85%|91.0%

最后补充一个例子，假设你在 “美国” 、 “印度” 、 “中国” 和 “其它地区” 这四个关键市场追踪你的猫分类器准确率，并且获得了四个指标。通过对这四个指标取平均值或进行加权平均，你将得到一个单值指标。**取平均值或者加权平均值是将多个指标合并为一个指标的最常用方法之一。**

# 优化指标和满意度指标
下面我们来了解一下组合多个评估指标的另一种方法。

假设你既关心学习算法的准确率（accuracy），又在意其运行时间（running time），请从下面的三个分类器中做出选择：

classifier|accuracy|running time
:----:|:----:|:----:|:----:
A|90%|80ms
B|92%|95ms
C|95%|1500ms

将准确率和运行时间放入单个公式计算后可以导出单个的指标，这似乎不太合理：
$$accuracy - 0.5 * running time$$

有一种替代方案可供选择：首先定义一个 “可接受的” 运行时间，一般低于 100ms 。接着，在限定的运行时间范围内，尽可能地将分类器的准确率最大化。此时，运行时间代表着 **“满意度指标”** —— 你的分类器必须在这个指标上表现得 “足够好” ，这里指的是运行时间约束上限为100ms；而准确度则代表着 **“优化指标”**。

再举一个例子，假设你正在设计一个硬件设备，该设备可以根据用户设置的特殊 “唤醒词”来唤醒系统，类似于 Amazon Echo 的监听词为 “Alexa”，苹果（Apple） Siri 的监听词为“Hey Siri”，安卓（Android） 的监听词为 “Okay Google”，以及百度（Baidu）应用的监听词“Hello Baidu.” 我们关心的指标是假正例率（false positive rate）—— 用户没有说出唤醒词，系统却被唤醒了，以及假反例率（false negative rate）——用户说出了唤醒词，系统却没能正确被唤醒。这个系统的一个较为合理的优化对象是尝试去最小化假反例率（优化指标），减少用户说出唤醒词而系统却没能正确唤醒的发生率，同时设置约束为每 24 小时不超过一次误报（满意度指标）。

一旦你的团队在优化评估指标上保持一致，他们将能够取得更快的进展。

# 通过开发集和度量指标加速迭代
对于当前面临的新问题，我们很难提前知道使用哪种方法会是最合适的，即使是一个经验丰富的机器学习研究员，通常也需要在尝试多种多样的方法之后才能发现令人满意的方案。当我要建立一个机器学习系统时，往往会这么做：

1、尝试一些关于系统构建的想法
2、使用diam实现想法
3、根据实验结果判断想法是否行得通。（第一个想到的点子一般都行不通！）在此基础上学习总结，从而产生新的想法，并保持这一迭代过程。迭代过程如下图所示：

![](/images/系统迭代.png)

迭代过程循环得越快，你也将进展得越快。此时，拥有开发集、测试集和度量指标的重要性便得以体现了：每当你有了一个新想法，在开发集上评估其性能就可以帮助你判断当前的方向是否正确。

假如你没有一个特定的开发集和度量指标，则需要在每次开发新的分类器时把它整合到 app中，并通过几个小时的体验来了解分类器的性能是否有所改进——这会浪费大量的时间！另外，如果你的团队将分类器的准确率从 95.0％ 提高到 95.1％，这 0.1% 的提升可能很难被检测出来。但是积少成多，通过不断积累这 0.1％ 的改进，你的系统将取得巨大的提升。拥有开发集和度量指标，可以使你更快地检测出哪些想法给系统带来了小（或大）的提升 ，从而快速确定下一步要研究或者是要放弃的方向。

# 何时修改开发集、测试集和指标
每当开展一个新项目时，我会尽快选好开发集和测试集，因为这可以帮团队制定一个明确的目标。

我通常会要求我的团队在不到一周（一般不会更长）的时间内给出一个初始的开发集、测试集和指标，提出一个不太完美的方案并迅速执行 ，这比起花过多的时间去思考要好很多。但是一周的时间要求并不适用于成熟的应用程序，譬如垃圾邮件过滤。我也见到过一些团队在已经成熟的系统上花费数月的时间来获得更好的开发集和测试集。

如果你渐渐发现初始的开发集、测试集和指标设置与期望目标有一定差距，那就尽快想办法去改进它们。例如你的开发集与指标在排序时将分类器 A 排在 B 的前面，然而你的团队认为分类器 B 在实际产品上的表现更加优异，这个时候就需要考虑修改开发集和测试集，或者是你的评估指标了。

在上述例子中，有三个主要原因可能导致开发集/评估指标错误地将分类器 A 排在 B 前面：

1、你需要处理的实际数据的分布和开发集/测试集的分布情况不同

假设你的初始开发集和测试集中主要是成年猫的图片，然而你在 app 上发现用户上传的更多是小猫的图片，这就导致了开发集和测试集的分布与你需要处理数据的实际分布情况不同。在这种情况下，需要更新你的开发集和测试集，使之更具代表性。

2、算法在开发集上过拟合了

在开发集上反复评估某个想法会导致算法在开发集上 “过拟合” 。当你完成开发后，应该在测试集上评估你的系统。如果你发现算法在开发集上的性能比测试集好得多，则表明你很有可能在开发集上过拟合了。在这种情况下，你需要获取一个新的开发集。

3、该指标不是项目应当优化的目标

假设你的猫咪 app 当前的指标为分类准确率，而该指标认为分类器 A 优于分类器 B。然而在尝试了两种算法后，你发现分类器 A 竟然允许出现一些色情图片，这实在是难以容忍。应该怎么办呢？

以上这种情况表明，此时的指标并不能辨别出算法 B 在实际产品中的表现是否比 A 更好，因此根据该指标来选择算法并不可靠，也说明此时应该改变现有的评估指标。你可以选择修改指标，使之对出现色情图片的情况执行严重惩罚。此外，强烈建议你选择一个新的指标并为你的团队制定一个新的研究目标，而不是在不可信的指标上耗费太多的时间，最终导致不得不回过头对分类器进行人工选择。


在项目中改变开发集、测试集或者指标是很常见的。一个初始的开发集、测试集和指标能够帮助团队进行快速迭代，当你发现它们对团队的导向不正确时，不要担心，你只需要对其进行修改并确保团队能够了解接下来的新方向。

# 小结
1、被选择作为开发集和测试集的数据，应当与你未来计划获取并对其进行良好处理的数据有着相同的分布，而不一定和训练集的数据分布一致

2、开发集和测试集的分布应当尽可能一致

3、为你的团队选择一个单值评估指标进行优化

4、当需要考虑多项目标时，不妨将它们整合到一个表达式里（比如对多个误差指标取平均），或者设定满意度指标和优化指标。

5、机器学习是一个高度迭代的过程：在出现最终令人满意的方案之前，你可能要尝试很多想法。

6、拥有开发集、测试集和单值评估指标可以帮助你快速评估一个算法，从而加速迭代进程。

7、当你要探索一个全新的应用时，尽可能在一周内建立你的开发集、测试集和评估指标；而在已经相对成熟的应用上，可以考虑花费更长的时间来执行这些工作。

8、传统的 70% / 30% 训练集/测试集划分对于大规模数据并不适用，实际上，开发集和测试集的比例会远低于 30%

9、开发集的规模应当大到能够检测出算法精度的细微改变，但也不需要太大；测试集的规模应该大到能够使你能对系统的最终性作出一个充分的估计。

10、当开发集和评估指标对于团队已经不能提供一个正确的导向时，尽快修改它们：
(i) 如果算法在开发集上过拟合，则需要获取更多的开发集数据。
(ii) 如果开发集与测试集的数据分布和实际数据分布不同，则需要获取新的开发集和测试集。
(iii) 如果评估指标无法对最重要的任务目标进行度量，则需要修改评估指标。
