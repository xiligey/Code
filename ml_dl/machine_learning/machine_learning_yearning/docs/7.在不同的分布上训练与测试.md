## [目录]

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [[目录]](#目录)
- [何时在不同的分布上训练与测试](#何时在不同的分布上训练与测试)
- [如何决定是否使用你所有的数据](#如何决定是否使用你所有的数据)
- [如何决定是否添加不一致的数据](#如何决定是否添加不一致的数据)
- [给数据添加权重](#给数据添加权重)
- [从训练集泛化到开发集](#从训练集泛化到开发集)
- [区分偏差、方差和数据不匹配误差](#区分偏差-方差和数据不匹配误差)
- [解决数据不匹配问题](#解决数据不匹配问题)
- [人工合成数据](#人工合成数据)

<!-- /code_chunk_output -->

## 何时在不同的分布上训练与测试

假设用户已经向你的猫咪图片程序上传了 10000 张图片，且图片已被人为标记为含有猫与不含猫两类。同时你也从互联网上下载了规模更大的 200000 张图片集，此时训练集、测试集与开发集应该如何定义呢？

这 10000 张用户上传的图片密切地反映了你想要处理的数据的实际概率分布，因此你可以将它们作为开发集与测试集，但如果你在训练一个饥渴的深度学习算法，需要提供大量的训练数据给它，那你就需要那 200000 张从网上下载的图片。但是这样面临了一个问题：训练集和开发测试集的分布不一致了。

大多数机器学习的学术文献都假设训练集、开发集和测试集是同一个分布。

在机器学习的早期，数据是稀缺的。我们通常只有一个服从某些概率分布的数据集 11。因此，我们会随机地将这些数据分割成训练/开发/测试集，并且假设所有的数据来源相同且满足要求。**但在大数据时代，我们现在可以使用大型的训练集，比如猫的网络图像。即使训练集的分布不同，我们仍然希望使用它来学习，因为它可以提供大量的信息**。

对于猫咪检测器的示例，我们不会将用户上传的所有 10000 个图像放到开发/测试集合中，而是将其中 5000 张放入。 剩下的 5000 张和互联网上的 200000 张放入训练集。

## 如何决定是否使用你所有的数据

在使用早期的学习算法（比如人为设计的计算机视觉特征，然后使用一个简单的线性分类器）时，真正的风险在于：合并这两种类型的数据会导致算法的表现更差。因此，一些工程师会警告你不要加入 20000 张互联网图片。

但是有了现代强大而灵活的学习算法——比如大型的神经网络——这种风险已经大大降低了。如果你能够构建一个有足够多的隐藏单元/层的神经网络，你可以安全地将 20000 张图片添加到你的训练集。此时添加图片则更有可能提升算法的性能。这种观察依赖于这样一个事实，即有一些 x-y 映射对于这两种类型的数据都很有效。换而言之，有这么些系统可以输入互联网图像或移动应用上的图像，并可靠地预测标签，即使它不知道图像的来源。

添加额外的 20000 张图片会产生以下影响：

- 它给你的神经网络提供了更多关于猫咪外貌的样本。这是很有帮助的，因为互联网图片和用户上传的移动应用图片确实有一些相似之处。你的神经网络可以将从互联网图像中获得的一些知识应用到移动应用图像中。
- 它迫使神经网络花费部分容量来学习网络图像的特定属性（比如更高的分辨率，不同画面结构图像的分布等等）。如果这些属性与移动应用图像有很大的不同，那么它将“耗尽”神经网络的一些表征能力，导致从移动应用图像的分布识别数据的能力就会降低，而这正是你真正关心的东西。从理论上讲，这可能会损害算法的性能。

但是，如果你没有足够大的神经网络（或者另一个高度灵活的学习算法），那么你应该更加关注训练数据，需要与开发集/测试集的分布相匹配。

如果你认为有些数据没有任何帮助，那么应该将这些数据排除在计算原因之外。例如，假设你的开发/测试集主要包含一些内容是人员、地点、地标、动物的任意图片。同时假设里面有大量的历史文档扫描图片，这些文件不包含任何类似猫的东西。它们看起来和开发/测试集的分布完全不同。没有必要将这些数据作为负样本，因为上述第一个影响带来的好处在这种情况下几乎忽略不计——你的神经网络几乎没有任何东西可以从这些数据中学习，但它们可以应用到开发/测试集中，加入它们将会浪费计算资源和神经网络的表征能力。

## 如何决定是否添加不一致的数据

假设你想要学习预测纽约市的房价。考虑房子的大小（输入特征 x），你需要预测价格（目的标签 y）。纽约市的房价非常高。假设你在密歇根州的底特律有第二个住房价格数据集，就会发现那里的房价要低得多。应该把这些数据包含在你的训练集里吗？

房子的大小 x 相同，而价格 y 明显不同，这取决于它是在纽约还是在底特律。如果你只关心预测纽约市的房价，把这两个数据集放在一起会影响算法的表现。在这种情况下，最好忽略不一致的底特律数据 13。

纽约和底特律的样本与移动应用和互联网猫图片的样本有什么不同？

猫咪图像的样本和这有点不一样，因为给定一个输入图片 x ，你能可靠地预测出标签 y （是否有猫），即使不知道图像是网络图像还是移动应用图像。即有一个函数 f（x）可以从输入 x 映射到目标输出 y ，即使不知道 x 的来源。 因此，从互联网图像中识别的任务与移动应用图像识别的任务是“一致的”。这意味着，将所有的数据包括在内，几乎没有什么负面影响（除了计算成本），甚至还可能有一些积极的作用。相比之下，纽约和底特律的数据则不一致。考虑相同的 x（房子的大小），价格会根据房子的位置而不同。

有一种方法可以解决底特律的数据与纽约市数据不一致的问题，即在每一个显示城市的训练样本中增加一个额 外的特征。给定一个输入 x ——代表所在的城市—— 此时目标值 y 是明确的。然而在实践中，我并不经常看到 这种情况。

## 给数据添加权重

假设你有 20 万张来自互联网的图片，还有来自移动应用用户的 5000 张照片。数据集的大小之间有一个 40:1 的比率。从理论上讲，只要你建立了一个庞大的神经网络，并在所有 205000 张图片上进行足够长的时间训练，那么在网络图像和移动图像上将算法都训练得很好是没有害处的。

但在实际操作中，拥有 40 倍的网络图像可能意味着，相比只使用 5000 张图片，你需要花费 40 倍（或更多）的计算资源来对两者进行建模。如果你没有巨大的计算资源，你可以给互联网图片一个较低的权重作为妥协。

例如，假设优化目标是平方误差（对于分类任务来说这不是一个好的选择，但它将简化解释过程）。因此，我们的学习算法试图优化：

$$min_{\theta}\sum_{(x,y)\in MobileImg}(h_{\theta}-y)^2+\sum_{(x,y)\in InternetImg}(h_{\theta}-y)^2$$

上面的第一个项是对 5000 个移动应用图像误差求和，第二项对 20 万个互联网图像误差求和。你可以使用一个额外的参数 𝛽 进行优化：

$$min_{\theta}\sum_{(x,y)\in MobileImg}(h_{\theta}-y)^2+\beta \sum_{(x,y)\in InternetImg}(h_{\theta}-y)^2$$

如果你设置$\beta=\frac 1 {40}$，这个算法会对 5000 个移动图像和 20 万个互联网图像给予同等的权重。你还可以将参数 $\beta$ 设置为其他值，也可以类似地对开发集进行调优

通过对额外的网络图像赋予更少的权重，你不需要构建一个庞大的神经网络来确保算法在这两种类型的任务上都能很好地完成。只有当你怀疑这些额外的数据（网络图像）与开发/测试集分布不一致，或者额外的数据规模比与相同分布的开发/测试集（手机图像）数据规模大得多时，这种类型的权重加权才需要。

## 从训练集泛化到开发集

假设你正在将机器学习应用于不同分布的训练集和开发/测试集上。例如，训练集包含了互联网图像+移动应用图像，而开发/测试集只包含移动应用图像。然而，该算法运行得不太好：它的开发/测试集误差比想要的要高得多。以下是一些可能出现问题的情况：

- 它在训练集上表现不佳，这属于训练集分布上的高（可避免）偏差的问题。
- 它在训练集上做得很好，但是不能很好地泛化到与训练集分布相同的未知数据，这是高方差问题。
- 它能够很好地泛化到与训练集相同分布的未知数据，但不能很好地泛化到与开发/测试集相同分布的未知数据。我们将这种情况称之为数据不匹配，因为训练集的数据与开发/测试集的数据匹配得相当地糟糕。

例如，假设人类在猫识别任务上取得近乎完美的表现。你的算法实现了：

- 1% 的训练集误差
- 1.5% 的与训练集分布相同的未知数据上的误差
- 10% 的开发集误差

在这种情况下，显然存在着数据不匹配问题。为了解决这个问题，你可能会尝试使训练数据更类似于开发/测试数据。我们稍后将讨论一些相关技术。为了诊断一个算法在上面 1 到 3 个问题受到了多大程度的影响，存在另一个数据集将是很有

用的。具体地说，与其给算法提供所有可用的训练数据，你还可以把它分成两个子集：算法将

进行训练的实际训练集，以及一个单独的集合，我们称之为“训练开发”集，我们将不会对它进

行训练。

你现在有四个数据子集：

- 训练集：这是算法将学习的数据（例如，互联网图像+移动应用图像）。这并不需要我

们从与真正关心的相同分布（开发/测试集分布）的数据中提取。

- 训练开发集：这些数据来自与训练集相同的分布（例如，互联网图像+移动应用图

像）。它通常比训练集要小；它只需要足够大到来评估和跟踪我们的学习算法的进展。

- 开发集：这是从与测试集相同分布的数据中抽取出来的，它反映了我们最终关心的数据

的分布（例如，移动应用图像） 。

- 测试集：这是从与开发集相同分布的数据中抽取出来的（例如，移动应用图像）。

有了这四个独立的数据集，你现在可以评估：

- 训练误差，对训练集进行评估。

- 该算法能够泛化到与训练集相同分布数据的能力，并对训练开发集进行评估。

- 算法在你实际关心的任务上的性能，通过对开发集 和/或 测试集评估。

> 用于选择开发集大小的大多数指导原则也适用于训练开发集

## 区分偏差、方差和数据不匹配误差

**高方差举例：**
假设在猫咪检测任务中，人类获得了近乎完美的性能（0%误差），因此最优错误率大约为 0%。假设你有：

- 1% 的训练集误差
- 5% 的训练开发集误差
- 5% 的开发集误差

表明你有较高的方差

**高可避免偏差：**
现在，假设你的算法达到了：
- 10% 的训练集误差
- 11% 的训练开发集误差
- 12% 的开发集误差 

这表明你在训练集上有很高的可避免偏差。

**高方差 & 高可避免偏差 & 数据不匹配误差**
- 10% 的训练集误差
- 11% 的训练开发集误差
- 20% 的开发集误差

该算法存在高可避免偏差和数据不匹配问题。然而，它在训练集的分布上并没有很大的差异。
![](/images/方差与偏差.png)


继续以猫咪图像检测器为例，你可以看到在 x 轴上有两种不同的数据分布。在 y 轴上，我们有三种类型的误差：人为误差，算法上误差，以及算法未经过训练的样本误差。我们可以用我们在前一章中发现的不同类型的误差来填写表格。

如果你愿意，你也可以在这个表格中填入剩下的两个空格：你可以通过让一些人给你的手机图片数据贴上标签，并测量他们的误差，你可以填写右上角的空格（移动应用图像上的人类水平表现）。你也可以通过移动应用猫的图像（分布 B ）来填充下一个空格，并将一小部分放入训练集，这样神经网络也可以学习它。 然后在数据的子集上测量学习模型的误差。填充这两 个额外的条目可能会让我们对算法在两个不同的分布（分布 A 和 B ）上做的事情有更多的了解。

通过了解算法最容易产生哪些类型的误差，你将能够更好地决定是否聚焦于减少偏差、减少方差或减少数据不匹配的技术。

## 解决数据不匹配问题
假设你已经开发了一个语音识别系统，它在训练集和训练开发集上都做得很好。但是，它在你的开发集上做得很差：这表明有一个数据不匹配的问题。你会怎么做呢?

我建议你：
1. 尝试理解数据属性在训练集和开发集分布之间的差异。（

2. 尝试找到更多的训练数据，以便更好地匹配你的算法碰到的开发集样本。

例如，假设你在语音识别的开发集中进行误差分析：手动地遍历 100 个样本，并尝试理解算法错出在哪。你会发现你的系统做得的确很差，因为在开发集中，大部分的音频剪辑都是在一辆车里录制的，而大多数的训练样本都是在一个安静的环境下录制的。引擎和道路噪音极大地恶化了你的语音系统的性能。在这种情况下，你可能会尝试获得更多的训练数据，包括在汽车里拍摄的音频片段。误差分析的目的是了解训练集和开发集之间的显著差异，这正是导致数据不匹配的原因。

不幸的是，这个过程没有任何保证。例如，如果你没有任何方法获得更多的训练数据，来更好地匹配开发集数据，那么你可能没有一条明确的路径来提高性能。

## 人工合成数据

你的语音系统需要更多的数据，它们听起来就像是从车里录制得到的。与其在开车的时候收集大量的数据，不如通过人工合成数据来获取这些数据。

假设你获得了大量的汽车/道路噪音的音频剪辑。你可以从几个网站下载这些数据。假设你也有一群在安静的房间里说话的人。如果你把一个人的音频片段“添加”到一个汽车/道路噪音的音频片段，你会得到一个音频剪辑，听起来就好像那个人在嘈杂的汽车里说话一样。使用这个过程，你可以“合成”大量的数据，听起来就像是在汽车里收集的。
更一般的情况是，在一些情况下，人工合成数据允许你创建一个与开发集相当匹配的巨大数据集，让我们使用猫咪图像检测器作为第二个例子。你注意到，开发集的图像有更多的动态模糊，因为它们往往来自手机用户，他们在拍照时会微微地移动手机。你可以从网络图像的训练集 中获取非模糊的图像，并将模拟的动态模糊添加到它们中，从而使它们更类似于开发集。

请记住，人工数据合成存在一定的挑战：有时候创建一个对人而言真实的合成数据比创建对计算机而言真实的数据要容易得多。例如，假设你有 1000 小时的语音训练数据，但只有 1 小时的汽车噪音。如果你反复使用相同的 1 小时的汽车噪音，从最初的 1000 小时的训练数据中，你将会得到一个合成的数据集，然而同样的汽车噪音会不断重复。听这段音频的人可能无法分辨——所有的汽车噪音对我们大多数人来说都是一样的——但是某种学习算法可能会“过拟合”一小时的汽车噪音。因此，它可能无法很好地泛化到一个新的音频剪辑片段，里面汽车的噪音听起来是不同的。

另一种情况，假设你有 1000 个小时的汽车噪音片段，但所有的噪音都是从 10 辆不同的车上提取的。在这种情况下，一种算法可能会“过拟合”这 10 辆车，如果在不同的汽车上进行音频测试，性能则会很差。不幸的是，这些问题很难被发现。 再举一个例子，假设你正在建立一个计算机视觉系统来识别汽车：你正与一家电脑游戏公司合
作，该公司拥有几辆汽车的计算机图形模型。为了训练你的算法，你可以使用这些模型来生成汽车的合成图像。即使合成的图像看起来非常真实，但这种方法（已经被许多人独立提出）可能不会很好地工作。在整个电脑游戏中，可能有 20 种汽车设计。制造一辆汽车的 3D 模型价格是非常昂贵的；如果你在玩这个游戏，你可能不会注意到你正在一遍又一遍地看到同样的车，也许只是换了一种颜色。即这些数据对你来说很真实。但是，与所有在道路上行驶的汽车相 比——也就是你可能在开发/测试集里看到的——这组根据 20 辆汽车模型合成的汽车只捕获了世界上销售的汽车的极小一部分。因此，如果你的 10 万个训练样本都来自这 20 辆车，你的系统将会“过拟合”这 20 款特定的汽车设计，而且它将无法很好地泛化到包含其他汽车设计在内的开发/测试集。

当你在合成数据时，请考虑一下你是否真的在合成一组具有代表性的样本。尽量避免给出合成数据的属性，这将使学习算法有可能将合成和非合成的样本区分开来——例如，所有的合成数据是否来自 20 个汽车设计中的某一个，或者所有的合成音频是否都来自于某个小时的汽车噪音。这个建议很容易被忽视。 在处理数据合成过程时，我的团队有时会花上几周的时间来生成带有细节的数据，这些数据与实际的数据分布非常接近，从而产生显著的效果。但如果你能够正确地获取这些细节，你可以突然获得比以前更大的训练集。