[目录]
---

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [误差的两大来源：偏差和方差](#误差的两大来源偏差和方差)
- [偏差和方差举例](#偏差和方差举例)
- [与最优错误率比较](#与最优错误率比较)
- [处理偏差和方差](#处理偏差和方差)
- [偏差和方差间的权衡](#偏差和方差间的权衡)
- [减少可避免偏差的技术](#减少可避免偏差的技术)
- [训练集误差分析](#训练集误差分析)
- [减少方差的技术](#减少方差的技术)

<!-- /code_chunk_output -->



# 误差的两大来源：偏差和方差

假设你的训练集、开发集和测试集都来自相同的分布，那么每次你都应该试图去获取更多的训练数据，因为这样能单独提高性能，对吗？

拥有更多的数据是无害的，然而它并不总是如我们期望的那样有帮助。有时获取更多的数据可能是在浪费时间。那么，何时才应该添加数据呢？

机器学习中有两个主要的误差来源：偏差和方差。理解它们将协助你决定是否该添加数据，并依此合理安排时间去执行其它的策略来提升性能。

假设你希望构建一个误差为 5% 的猫识别器。而目前的训练集错误率为 15%，开发集错误率为 16%。在这种情况下，添加数据对结果可能不会有太多帮助。你应该关注其他改进策略。

实际上，在你的训练集上添加过多样本只会让你的算法难以在训练集上做的更好。（我们在后面章节中解释了原因）

如果你在训练集上的错误率是 15%（即 85% 的精度），但你的目标是 5% 错误率（95% 精度），那么首先要解决的问题是提高算法在训练集上的性能。算法在开发/测试集上的性能通常比在训练集上要差。所以，如果算法在已知样本上达到了 85% 的精度，那么是不可能在未知样本上达到 95% 精度的。

如上所述，假设你的算法在开发集上有 16% 的错误率（84% 精度），我们将这 16% 的错误率分为两部分：

- 第一部分是算法在训练集上的错误率。在本例中，它是 15%。我们非正式地将它作为算法的偏差（**bias**）。

- 第二部分指的是算法在开发集（或测试集）上的表现比训练集上差多少。在本例中，开发集表现比训练集差 1%。我们非正式地将它作为算法的方差（**variance**）。

在统计学领域有着更多关于偏差和方差的正式定义，但不必担心。粗略地说，偏差指的是算法在大型 训练集上的错误率；方差指的是算法在测试集上的表现低于训练集的程度。当你使用均方误差（MSE ）作为误差度量指标时，你可以写下偏差和方差对应的两个公式，并且证明总误差=偏差+方差。但在 处理机器学习问题时，此处给出的偏差和方差的非正式定义已经足够。

一些学习算法的优化能解决误差来源的第一个部分——偏差，并且提高算法在训练集上的性能

；而另一些优化能解决第二个部分——方差，并帮助算法从训练集到开发/测试集上更好地泛

化。为了选择最有成效的改变方向，了解二者哪一方更需解决是很有用的。

建立对偏差和方差的良好直觉将帮助你为算法选择出有效的改进策略。

# 偏差和方差举例

思考一下，我们的“猫分类”任务目标：一个“理想的”分类器（比如人类）在这个任务中能够取得近乎完美的表现。

假设你的算法表现如下：

- 训练错误率 = 1%
- 开发错误率 = 11%

这其中存在什么问题呢？根据前一章的定义，我们估计偏差为 1%，方差为 10%（=11%-1%

）。因此，它有一个很高的方差（**high variance**）。虽然分类器的训练误差非常低，但是并

没有成功泛化到开发集上。这也被叫做**过拟合**（**overfitting**）。

接下来，考虑如下情况：

- 训练错误率 = 15%
- 开发错误率 = 16%

我们估计偏差为 15%，方差为 1%。该分类器的错误率为 15%，没有很好地拟合训练集，但

它在开发集上的误差不比在训练集上的误差高多少。因此，该分类器具有较高的偏差（**high**

**bias**），而方差较低。我们称该算法是**欠拟合**（**underfitting**）的。

下面，考虑如下情况：

- 训练错误率 = 15%
- 开发错误率 = 30%

我们估计偏差为 15%，方差为 15%。该分类器有高偏差和高方差（**high bias and high**

**variance**）：它在训练集上表现得很差，因此有较高的偏差，而它在开发集上表现更差，因

此方差同样较高。由于该分类器同时过拟合和欠拟合，过拟合/欠拟合术语很难准确应用于

此。

最后，考虑如下情况：

- 训练错误率 = 0.5%
- 开发错误率 = 1%

该分类器效果很好，它具有低偏差和低方差。恭喜获得这么好的表现！

# 与最优错误率比较

在我们的“猫咪识别”案例中，“理想”错误率——即一个“最优”分类器应该达到的值——接近 0%

。在几乎所有情况下，人类总是可以识别出图片中的猫。因此，我们希望机器也能够有这样优

秀的表现。

若换作其他问题，难度则更大：假设你正在构建一个语音识别系统，并发现 14% 的音频片段

背景噪声太多，或者十分难以理解，导致即使是人类也无法识别出所说的内容。在这种情况下

，即使是“最优”的语音识别系统也可能约有 14% 的误差。

假设在这个语音识别问题上，你的算法达到：

- 训练错误率 = 15%
- 开发错误率 = 30%

算法在训练集上的表现已经接近最优错误率 14%，因此在偏差上或者说在训练集表现上没有

太大的提升空间。然而，算法没有很好地泛化到开发集上，在方差造成的误差上还有很大的提

升空间。

这个例子和前一章的第三个例子类似，都有 15% 的训练错误率和 30% 的开发错误率。如果

最优错误率接近 0%，那么 15% 的训练错误率则留下了很大的提升空间，这表明降低偏差可

能有益。但如果最优错误率是 14%，那么 15% 的训练错误率表现告诉我们，在分类器的偏差

方面几乎没有改进的余地。

对于最佳错误率远超零的状况，有一个对算法误差更详细的分解。继续使用上述语音识别的例

子，可以将 30% 的总开发集误差分解如下（类似的分析可以应用于测试集误差）：

- 最优错误率（**“**不可避免偏差**”**）：14%。假设我们决定，即使是世界上最好的语音系

统，仍会有 14% 的误差。我们可以将其认为是学习算法的偏差“不可避免”的部分。

- 可避免偏差：1%。即训练错误率和最优误差率之间的差值。8
- 方差：15%。即开发错误和训练错误之间的差值。

8 如果可避免偏差值是负的，即算法在训练集上的表现比最优错误率要好。这意味着你正在过拟合训练集，并且

算法已经过度记忆（over-memorized）训练集。你应该专注于有效降低方差的方法，而不是选择进一步减少偏差

的方法。

为了将这与我们之前的定义联系起来，偏差和可避免偏差关系如下：

$偏差 = 最佳误差率（“不可避免偏差”）+ 可避免的偏差$

使用这些定义是为了更好地帮助读者理解如何改进学习算法。这些定义与统计学家定义的偏差和方差不同。从技术角度上说，这里定义的“偏差”应该叫做“我们认为是偏差的误差”；另外“可避免偏差”应该叫做“我们认为学习算法的偏差超过最优错误率的误差”。

这个“可避免偏差”反映了算法在训练集上的表现比起“最优分类器”差多少。

方差的概念和之前保持一致。理论上来说，我们可以通过训练一个大规模训练集将方差减少到

接近零。因此只要拥有足够大的数据集，所有的方差都是可以“避免的”，所以不存在所谓的“不

可避免方差”。

再考虑一个例子，该例子中最优错误率是 14%，我们有：

- 训练误差 = 15%
- 开发误差 = 16%

我们在前一章称之为高偏差分类器，现在可避免的偏差误差是 1%，方差误差约为 1%。因此

，算法已经做的很好了，几乎没有提升的空间。它只比最佳错误率低 2%。

从这些例子中我们可以看出，了解最优错误率有利于指导我们的后续工作。在统计学上，最优

错误率也被称为**贝叶斯错误率**（**Bayes error rate**），或贝叶斯率。

如何才能知道最优错误率是多少呢？对于人类擅长的任务，例如图片识别或音频剪辑转录，你

可以让普通人提供标签，然后测评这些人为标签相对于训练集标签的精度，这将给出最优错误

率的估计。如果你正在解决一项人类也很难解决的问题（例如预测推荐什么电影，或向用户展

示什么广告），这将很难去估计最优错误率。

在[与人类水平对比](6.与人类水平对比.md)这一节中，我将更详细地讨论学习算法的表现和人类表现相比较的过程。

在前面几个章节中，你学习了如何通过查看训练集和开发集的错误率来估计可避免/不可避免

的偏差和方差。下一章将讨论如何据此来判断该优先减少偏差还是方差。项目当前的问题是高

偏差（可避免偏差）还是高方差，将导致你采用截然不同的方法。请继续阅读。

# 处理偏差和方差

下面是处理偏差和方差问题最简单的形式：

- 如果具有较高的可避免偏差，那么加大模型的规模（例如通过添加层/神经元数量来增加神经网络的大小）。

- 如果具有较高的方差，那么增加训练集的数据量。

如果你可以加大神经网络的规模且无限制地增加训练集数据，那么许多机器学习问题都可以取得很好的效果。

实际上，不断加大网络的规模使你终将遇到算力问题，因为训练一个大型模型需要很多时间。

另外，你也可能会耗尽获取更多训练数据的能力。（即使在网上，猫图片的数量也是有限的）

不同的模型架构（例如不同的神经网络架构）对于你的问题将有不同的偏差/方差值。近期，

不少深度学习研究已经开发出很多新的模型架构。所以，如果你在使用神经网络，学术文献可

能会是一个很好的灵感来源，在 Github 上也有许多不错的开源实现。但尝试新架构的结果要

比简单地加大模型规模或添加数据的形式更难以预测。

加大模型的规模通常可以减少偏差，但也可能会增加方差和过拟合的风险。然而，这种过拟合

风险通常只在你不使用正则化技术的时候出现。如果你的算法含有一个精心设计的正则化方法

，通常可以安全地加大模型的规模，而不用担心增加过拟合风险。

假设你正在应用深度学习方法，使用了 L2 正则化和 dropout 技术，并且设置了在开发集上表

现最好的正则化参数。此时你加大模型规模，算法的表现往往会保持不变或提升；它不太可能

明显地变差。这种情况下，不使用更大模型的唯一原因就是这将使得计算代价变大。

# 偏差和方差间的权衡

你可能听过“偏差和方差间的权衡”。目前，在大部分针对学习算法的改进中，有一些能够减少

偏差，但代价是增大方差，反之亦然。于是在偏差和方差之间就产生了“权衡”。

例如，加大模型的规模（在神经网络中增加神经元/层，或增加输入特征），通常可以减少偏

差，但可能会增加方差。另外，加入正则化一般会增加偏差，但能减少方差。

在现代，我们往往能够获取充足的数据，并且可以使用非常大的神经网络（深度学习）。因此

，这种权衡的情况比较少，并且现在有更多的选择可以在不损害方差的情况下减少偏差，反之

亦然。

例如，一般情况下，你可以通过增加神经网络的规模大小，并调整正则化方法去减少偏差，而

不会明显的增加方差。通过增加训练数据，你也可以在不影响偏差的情况下减少方差。

如果你选择了一个非常契合任务的模型架构，那么你也可以同时减少偏差和方差。只是选择这

样的架构可能有点难度。

在接下来的几个章节中，我们将讨论处理偏差和方差的其它特定技术。

# 减少可避免偏差的技术

如果你的学习算法存在着很高的可避免偏差，你可能会尝试以下方法：

- **加大模型规模**（例如神经元/层的数量）：这项技术能够使算法更好地拟合训练集，从而减少偏差。当你发现这样做会增大方差时，通过加入正则化可以抵消方差的增加。
- **根据误差分析结果修改输入特征**：假设误差分析结果鼓励你增加额外的特征，从而帮助算法消除某个特定类别的误差。（我们会在接下来的章节深入讨论这个话题。）这些新的特征对处理偏差和方差都有所帮助。理论上，添加更多的特征将增大方差；当这种情况发生时，你可以加入正则化来抵消方差的增加。
- **减少或者去除正则化**（L2 正则化，L1 正则化，dropout）：这将减少可避免偏差，但会增大方差。
- **修改模型架构**（比如神经网络架构）使之更适用于你的问题：这将同时影响偏差和方差。

有一种方法并不能奏效：

- **添加更多的训练数据**：这项技术可以帮助解决方差问题，但它对于偏差通常没有明显的影响。

# 训练集误差分析

算法必须首先在训练集表现的很好，才能期望它在开发集和测试集上也有良好的表现。

当你的算法存在高偏差(欠拟合)时，说明算法在训练集上表现并不佳，此时可以在训练集上设置一个Eyeball训练集，类似于开发集上设置的Eyeball开发集。

举例说明：

假设你正在为一个应用程序构建一个语音识别系统，并收集了一组志愿者提供的音

频片段。如果系统在训练集上表现不佳，你可能会考虑选取那些算法处理得很差的样本，以

100 个左右作为一组并人为去听它们，从而知道训练集误差的主要种类。类似于开发集上的误

差分析，你可以计算不同类别的错误样本数量：

![](/images/训练集误差分析.png)

在本例中，你可能会发现算法在处理具有大量背景噪音的训练样本时遇到了困难。因此你可能

会关注一些技术，使其能够更好地适应包含背景噪音的训练样本。

你也可以仔细检查正常人是否能转录这些音频片段，这些音频应该与你的学习算法的输入音频

相同。如果背景噪音过于嘈杂，导致任何人都无法理解音频里说了什么，那么期望算法正确地

识别这样的话语就不太合理。我们会在后面的章节中讨论，将算法的性能与人类水平进行比较

的好处。

# 减少方差的技术

如果你的学习算法存在着高方差问题(过拟合)，可以考虑尝试下面的技术：

- **添加更多的训练数据**：这是最简单最可靠的一种处理方差的策略，只要你有大量的数据和对应的计算能力来处理他们。
- **加入正则化**（L2 正则化，L1 正则化，dropout）：这项技术可以降低方差，但却增大了偏差。
- **加入提前终止**（例如根据开发集误差提前终止梯度下降）：这项技术可以降低方差但却增大了偏差。提前终止（Early stopping）有点像正则化理论，一些学者认为它是正则化技术之一。
- **通过特征选择减少输入特征的数量和种类**：这种技术或许有助于解决方差问题，但也可能增加偏差。稍微减少特征的数量（比如从 1000 个特征减少到 900 个）也许不会对偏差产生很大的影响，但显著地减少它们（比如从 1000 个特征减少到 100 个，10 倍地降低）则很有可能产生很大的影响，你也许排除了太多有用的特征。在现代深度学习研究过程中，当数据充足时，特征选择的比重需要做些调整，现在我们更可能将拥有的所有特征提供给算法，并让算法根据数据来确定哪些特征可以使用。而当你的训练集很小的时候，特征选择是非常有用的。
- **减小模型规模**（比如神经元/层的数量）：*谨慎使用。*这种技术可以减少方差，同时可能增加偏差。然而我不推荐这种处理方差的方法，添加正则化通常能更好的提升分类性能。 减少模型规模的好处是降低了计算成本，从而加快了你训练模型的速度。如果加速模型训练是有用的，那么无论如何都要考虑减少模型的规模。但如果你的目标是减少方差，且不关心计算成本，那么考虑添加正则化会更好。

下面是两种额外的策略，和解决偏差问题章节所提到的方法重复：

- **根据误差分析结果修改输入特征**：假设误差分析的结果鼓励你创建额外的特征，从而帮助算法消除某个特定类别的误差。这些新的特征对处理偏差和方差都有所帮助。理论上，添加更多的特征将增大方差；当这种情况发生时，加入正则化，这可以消除方差的增加。
- **修改模型架构**（比如神经网络架构）使之更适用于你的问题：这项策略将同时影响偏差和方差。